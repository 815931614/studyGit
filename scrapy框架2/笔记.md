### 创建Scrapy项目

- 创建scrapy项目
  - scrapy startproject mySpider

- 生成一个爬虫
  - cd  mySpider
  - scrapy genspider 爬虫名称  域名

- 启动爬虫

  scrapy crawl itcast

  



  

```
extract() 没有数据返回[]
extract_first() 和 get()  没有数据返回None
```

#### logging 模块的使用

- scrapy
  - settings中设置LOG_LEVEL = "WARNING"
  - settings中设置LOG_FILE="./a.log" # 设置日志保存的位置，设置后终端不会显示日志内容
- 普通项目中
  - import logging
  - logging.basicConfig(..) # 设置日志输出的样式，格式
  - 实例化一个logger=logging.getLogger(name)
  - 在任何py文件中调用logger即可

#### scrapy.Request知识点

​	**scrapy.Request(url**[,**callback**, method="GET",headers,body,cookies,**meta**,**dont_filter=False]**)

#### scrapy.Request常用参数为:

​	callback:指定传入的url交给哪个解析函数去处理

​	meta: 实现在不同的解析函数中传递数据，meta默认会携带部分信息，比如下载延迟，请求深度等	

​    dont_filter:让scrapy的去重不会过滤当前url，scrapy默认有url去重的功能，对需要重复请求的url有重要用途







#### Scrapy深入Scrapy_shell

```
Scrapy shell 是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPATH表达式

使用方法
scrapy shell url地址

response.url: 当前响应的url地址
response.request.url: 当前响应对应的请求的url地址
response.headers: 响应头
response.body:响应体，也就是html代码，默认是byte类型
response.requests.headers: 当前响应的请求头

```

正则

  re.findall("\"bp\":'(.*?)'")

### Scrapy中CrawlSpider

-   生成crawlspider的命令
  - scrapy genspider -t crawl csdn "csdn.cn"
- 注意点：
  1. 用命令创建一个crawlspider的模板：scrapy genspider -t crawl <爬虫名称> <all_domain>, 也可以手动创建
  2. CrawlSPider中不能再有以parse为名字的数据提取方法，这个方法被CrawlSpider用来实现基础url提取等功能
  3. 一个Rule对象接收很多参数，首先第一个包含url规则的LinkExtractor对象，常用的还有callback(制定满足规则的url的解析函数的字符串)和follow(response中提取的链接是否需要跟进)
  4. 不指定callback函数的请求下，如果follow为True，满足该rule的url还会继续被请求
  5. 如果多个Rule都满足某一个url，会从rules中选择第一个满足的进行操作
- LinkExtractor常见参数：
  - allow：满足括号中"正则表达式"的URL会被提取，如果为空，则全部匹配
  - deny：满足括号中“正则表达式”的URL一定不提取（优先级高于allow）
  - allow_domains: 会被提取的链接的domains
  - restrict_xpaths:使用xpath表达式，和allow共同作用过滤链接，即xpath满足范围内的url地址会被提取
- spider.Rule常见参数：
  - link_extractor: 是一个Line Extractor对象，用于定义需要提取的链接
  - callback：从link_extractor中每获取到链接时，参数锁指定的值作为回调函数
  - follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。
    - 如果callback为None，follow默认值为True，否则默认为FALSE
  - process_links: 指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数，该方法主要用来过滤url。
  - process_request:指定该spider中哪个的函数将会被调用，该规则提取到每个request时都会调用该函数

，用来过滤request



### scrapy模拟登录

- 应用场景：
  1. cookie过期时间很长，常见于一些不规范的网站
  2. 能在cookie过期之前把所有的数据拿到
  3. 配合其他程序使用，比如其使用selenium把登录之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie。

- COOKIES_DEBUG = True  查看cookie信息，LOG等级不能高于DEBUG





#### 下载器中间件

- 使用方法

  - 编写一个Downloader Middlewares，定义一个类，然后再settings中开启

- Downloader Middlewares默认的方法

  ```
  process_request(self,request,spider):
  	当每个request通过下载器中间件时，该方法被调用
  process_response(self.request,response,spider):
  	当下载器完成http请求，传递响应给引擎的时候调用
  ```

- 案例

  ```Python
  class RandomUserAgent(object):
  	def process_request(self, request, spider):
  		useragent = random.choice(USER_AGENTS)
  		request.headers["User-Agent"] = useragent
  		# 添加自定义的UA，给request的headers赋值即可
  
          
  class ProxyMiddleware(object):
      def process_request(self,request,spider):
          request.meta['proxy'] = 'http://124.115.123:9999'
  		# 添加代理，需要在request的meta信息中添加proxy字段
          # 代理的形式为：协议+ip地址+端口
  ```

  

### post请求

```Python
def parse(self, response):
        authenticity_token = response.xpath('//*[@id="login"]/div[4]/form/input[1]/@value').extract_first()
        timestamp_secret = response.xpath('//*[@id="login"]/div[4]/form/div/input[11]/@value').extract_first()
        timestamp = response.xpath('//*[@id="login"]/div[4]/form/div/input[10]/@value').extract_first()
        post_data = {
            'commit' : 'Sign in',
            'authenticity_token' : authenticity_token,
            'login' : '815931614@qq.com',
            'password' : 'qwe7410852963.',
            'trusted_device' : '',
            'webauthn-support' : 'supported',
            'webauthn-iuvpaa-support' : 'unsupported',
            'return_to' : 'https://github.com/login',
            'allow_signup' : '',
            'client_id' : '',
            'integration' : '',
            'required_field_4210' : '',
            'timestamp' : timestamp,
            'timestamp_secret' : timestamp_secret,
        }
        yield scrapy.FormRequest(
            'https://github.com/session',
            formdata=post_data,
            callback=self.after_login
        )


def after_login(self,response):
        import re
        print(re.findall('815931614',response.text))
        
        
        
---------------------------------------------------------------------------        
def parse(self, response):
    yield scrapy.FormRequest.from_response(
        response, # 自动重response中寻找from表单
        formdata={
            'login' : '815931614@qq.com',
            'password' : 'qwe7410852963.',
        },
        callback= self.after_login
    )
def after_login(self,response):
    import re
    print(re.findall('815931614',response.text))

```

url